{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from lib.envs.walking_cont import GridworldEnv\n",
    "from lib import plotting\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search action by epsilon greedy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action. Float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #should return function!!!\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype= float) * epsilon/nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0-epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-value update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy\n",
    "    while following an epsilon-greedy policy\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance to sample a random action. Float between 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, episode_lengths).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "        \n",
    "        # One step in the environment\n",
    "        # total_reward = 0.0\n",
    "        for t in range(0,5000): #itertools.count():\n",
    "            \n",
    "            # Take a step\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # TD Update\n",
    "            best_next_action = np.argmax(Q[next_state])    \n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 100/100."
     ]
    }
   ],
   "source": [
    "#run q learning\n",
    "Q, statsqlearning = q_learning(env, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "        action_probs = policy(state)\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in range(0,5000): #itertools.count():\n",
    "            # Take a step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Pick the next action\n",
    "            # in Q learning here is the best_next_action = argmax[next_state]\n",
    "            next_action_probs = policy(next_state)\n",
    "            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # TD Update\n",
    "            td_target = reward + discount_factor * Q[next_state][next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            action = next_action\n",
    "            state = next_state        \n",
    "    \n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 100/100."
     ]
    }
   ],
   "source": [
    "#run sarsa\n",
    "Q, statssarsa = sarsa(env, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEstimator():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, scope=\"policy_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
    "            self.action = tf.placeholder(dtype=tf.int32, name=\"action\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just table lookup estimator\n",
    "            state_one_hot = tf.one_hot(self.state, int(env.observation_space.n))\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(state_one_hot, 0),\n",
    "                num_outputs=env.action_space.n,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "\n",
    "            self.action_probs = tf.squeeze(tf.nn.softmax(self.output_layer))\n",
    "            self.picked_action_prob = tf.gather(self.action_probs, self.action)\n",
    "\n",
    "            # Loss and train op\n",
    "            self.loss = -tf.log(self.picked_action_prob) * self.target\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        return sess.run(self.action_probs, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, action, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = { self.state: state, self.target: target, self.action: action  }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEstimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, scope=\"value_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just table lookup estimator\n",
    "            state_one_hot = tf.one_hot(self.state, int(env.observation_space.n))\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(state_one_hot, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "\n",
    "            self.value_estimate = tf.squeeze(self.output_layer)\n",
    "            self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())        \n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        return sess.run(self.value_estimate, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = { self.state: state, self.target: target }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Actor Critic Algorithm. Optimizes the policy \n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator_policy: Policy Function to be optimized \n",
    "        estimator_value: Value function approximator, used as a critic\n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: Time-discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the fisrst action\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in range(0,5000): # itertools.count():\n",
    "            \n",
    "            # Take a step\n",
    "            action_probs = estimator_policy.predict(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Calculate TD Target\n",
    "            value_next = estimator_value.predict(next_state)\n",
    "            td_target = reward + discount_factor * value_next\n",
    "            td_error = td_target - estimator_value.predict(state)\n",
    "            \n",
    "            # Update the value estimator\n",
    "            estimator_value.update(state, td_target)\n",
    "            \n",
    "            # Update the policy estimator\n",
    "            # using the td error as our advantage estimate\n",
    "            estimator_policy.update(state, td_error, action)\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Wai Yan\\Anaconda3\\envs\\reinforcement_learning_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-10-3accf08a4b7a>:28: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "WARNING:tensorflow:From C:\\Users\\Wai Yan\\Anaconda3\\envs\\reinforcement_learning_env\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Wai Yan\\Anaconda3\\envs\\reinforcement_learning_env\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Step 293 @ Episode 100/100 (-964.0))"
     ]
    }
   ],
   "source": [
    "#run actor critic\n",
    "tf.reset_default_graph()\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "policy_estimator = PolicyEstimator()\n",
    "value_estimator = ValueEstimator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # Note, due to randomness in the policy the number of episodes you need to learn a good\n",
    "    # policy may vary. ~300 seemed to work well for me.\n",
    "    statsac = actor_critic(env, policy_estimator, value_estimator, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x1 = statsqlearning.episode_rewards\n",
    "x2 = statssarsa.episode_rewards\n",
    "x3 = statsac.episode_rewards\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.boxplot([x for x in [x1, x2, x3]], 0, 'rs', 1, showfliers = False)\n",
    "plt.xticks([y+1 for y in range(len([x1, x2, x3]))], ['x1', 'x2', 'x3'])\n",
    "plt.xlabel('measurement x')\n",
    "t = plt.title('Box plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x23b31f899e8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Q-learning\n",
      "Best -146.0\n",
      "Worst -1721.0\n",
      "Median -706.5\n",
      "Standard Deviation 341.64007450100246\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "print(\"Stats for Q-learning\")\n",
    "print(\"Best\", max(x1))\n",
    "print(\"Worst\", min(x1))\n",
    "print(\"Median\", statistics.median(x1))\n",
    "print(\"Standard Deviation\", statistics.stdev(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for SARSA\n",
      "Best -86.0\n",
      "Worst -1767.0\n",
      "Median -737.5\n",
      "Standard Deviation 365.066558757759\n"
     ]
    }
   ],
   "source": [
    "print(\"Stats for SARSA\")\n",
    "print(\"Best\", max(x2))\n",
    "print(\"Worst\", min(x2))\n",
    "print(\"Median\", statistics.median(x2))\n",
    "print(\"Standard Deviation\", statistics.stdev(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Actor Critic\n",
      "Best -101.0\n",
      "Worst -2335.0\n",
      "Median -724.0\n",
      "Standard Deviation 504.2595688009031\n"
     ]
    }
   ],
   "source": [
    "print(\"Stats for Actor Critic\")\n",
    "print(\"Best\", max(x3))\n",
    "print(\"Worst\", min(x3))\n",
    "print(\"Median\", statistics.median(x3))\n",
    "print(\"Standard Deviation\", statistics.stdev(x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
